{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1: Basic Model Serving\n",
    "\n",
    "This part of the real-time pipeline will first cover the basics of a model serving\n",
    "\n",
    "By the end of this tutorial you'll learn how to\n",
    "\n",
    "- Create model-serving functions.\n",
    "- Deploy models at scale.\n",
    "- Test your deployed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-prerequisites\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The following steps assume that you already have a model Classifier Model trained for Iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-step-setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-import-libaries\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "Run the following code to import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-set-mlrun-envr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Your MLRun Environment\n",
    "\n",
    "Use the `get_or_create_project` MLRun method to create a new project or fetch it from the DB/repository if it already exists.\n",
    "Set the `project` and `user_project` parameters to the same values that you used in the call to this method in the [Part 1: MLRun Basics](./01-mlrun-basics.ipynb#gs-tutorial-1-mlrun-envr-init) tutorial notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-05-25 20:27:23,875 [info] loaded project realtime-pipeline from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "# Set the base project name\n",
    "project_name_base = 'realtime-pipeline'\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name_base, context=\"./\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-step-writing-a-serving-class\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Writing A Simple Serving Class\n",
    "\n",
    "The serving class is initialized automatically by the model server.\n",
    "All you need is to implement two mandatory methods:\n",
    "\n",
    "- `load` &mdash; downloads the model files and loads the model into memory.\n",
    "    This can be done either synchronously or asynchronously.\n",
    "- `predict` &mdash; accepts a request payload and returns prediction (inference) results.\n",
    "\n",
    "For more detailed information on serving classes, see the [MLRun documentation](https://github.com/mlrun/mlrun/blob/release/v0.6.x-latest/mlrun/serving/README.md).\n",
    "\n",
    "The following code demonstrates a minimal scikit-learn (a.k.a. sklearn) serving-class implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudpickle import load\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import mlrun\n",
    "\n",
    "class ClassifierModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model('.pkl')\n",
    "        self.model = load(open(model_file, 'rb'))\n",
    "\n",
    "    def predict(self, body: dict) -> List:\n",
    "        \"\"\"Generate model predictions from sample.\"\"\"\n",
    "        feats = np.asarray(body['inputs'])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-step-deploy-the-serving-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploying the Model-Serving Function (Service)\n",
    "\n",
    "To provision (deploy) a function for serving the model (\"a serving function\") you need to create an MLRun function of type `serving`.\n",
    "You can do this by using the `code_to_function` MLRun method from a web notebook, or by importing an existing serving function or template from the MLRun functions marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-convert-serving-class-to-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a Serving Class to a Serving Function\n",
    "\n",
    "The following code converts the `ClassifierModel` class that you defined in the previous step to a serving function.\n",
    "The name of the class to be used by the serving function is set in `spec.default_class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = mlrun.code_to_function('serving', kind='serving',image='mlrun/mlrun')\n",
    "serving_fn.spec.default_class = 'ClassifierModel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the model created in previous notebook by the training function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = project.get_artifact_uri('my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store://artifacts/realtime-pipeline-xingsheng/my_model\n"
     ]
    }
   ],
   "source": [
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskStep at 0x7fbb867fba90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.add_model('my_model',model_path=model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Function Locally\n",
    "\n",
    "To test your function locally, create a test server (mock server) and test it with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_data = '''{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-05-25 22:39:22,479 [info] model my_model was loaded\n",
      "> 2022-05-25 22:39:22,480 [info] Loaded ['my_model']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'c31d5783a5c44d54b671d9162a141d1f',\n",
       " 'model_name': 'my_model',\n",
       " 'outputs': [0, 2]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server = serving_fn.to_mock_server()\n",
    "server.test(\"/v2/models/my_model/infer\", body=my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-building-and-deploying-the-serving-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Deploying the Serving Function\n",
    "\n",
    "Use the `deploy` method of the MLRun serving function to build and deploy a Nuclio serving function from your serving-function code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-05-25 22:42:00,386 [info] Starting remote function deploy\n",
      "2022-05-25 22:42:00  (info) Deploying function\n",
      "2022-05-25 22:42:00  (info) Building\n",
      "2022-05-25 22:42:00  (info) Staging files and preparing base images\n",
      "2022-05-25 22:42:00  (info) Building processor image\n",
      "2022-05-25 22:42:45  (info) Build complete\n",
      "2022-05-25 22:42:53  (info) Function deploy complete\n",
      "> 2022-05-25 22:42:53,872 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-realtime-pipeline-xingsheng-serving.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['realtime-pipeline-xingsheng-serving-realtime-pipeline-xingsheng.default-tenant.app.us-sales-341.iguazio-cd1.com/']}\n"
     ]
    }
   ],
   "source": [
    "function_address = serving_fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-step-using-the-live-serving-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Using the Live Model-Serving Function\n",
    "\n",
    "After the function is deployed successfully, the serving function has a new HTTP endpoint for handling serving requests.\n",
    "The example tutorial serving function receives HTTP prediction (inference) requests on this endpoint;\n",
    "calls the `infer` method to get the requested predictions; and returns the results on the same endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The address for the function is http://realtime-pipeline-xingsheng-serving-realtime-pipeline-xingsheng.default-tenant.app.us-sales-341.iguazio-cd1.com/ \n",
      "\n",
      "{\"name\": \"ModelRouter\", \"version\": \"v2\", \"extensions\": []}"
     ]
    }
   ],
   "source": [
    "print (f'The address for the function is {function_address} \\n')\n",
    "\n",
    "!curl $function_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-testing-the-model-server\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model Server\n",
    "\n",
    "Test your model server by sending data for inference.\n",
    "The `invoke` serving-function method enables programmatic testing of the serving function.\n",
    "For model inference (predictions), specify the model name followed by `infer`:\n",
    "```\n",
    "/v2/models/{model_name}/infer\n",
    "```\n",
    "For complete model-service API commands &mdash; such as for list models (`models`), get model health (`ready`), and model explanation (`explain`) &mdash; see the [MLRun documentation](https://github.com/mlrun/mlrun/blob/release/v0.6.x-latest/mlrun/serving/README.md#model-server-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-05-25 22:43:34,935 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-realtime-pipeline-xingsheng-serving.default-tenant.svc.cluster.local:8080/v2/models/my_model/infer'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '1ca676fd-28d9-4310-a77e-dd4864096598',\n",
       " 'model_name': 'my_model',\n",
       " 'outputs': [0, 2]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.invoke('/v2/models/my_model/infer', my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-3-step-view-serving-func-in-ui\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Viewing the Nuclio Serving Function on the Dashboard\n",
    "\n",
    "On the **Projects** dashboard page, select the project and then select \"Real-time functions (Nuclio)\".\n",
    "\n",
    "![Nuclio](./images/nuclio-deploy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
